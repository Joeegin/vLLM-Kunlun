name: Unit Test

on:
  pull_request:
    branches:
      - main

jobs:
  test-kunlun:
    runs-on:
      labels:
        - self-hosted
        - Linux
        - X64

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Start Docker
        run: |
          set -eux
          docker rm -f vllm-ut || true
          XPU_NUM=8
          DOCKER_DEVICE_CONFIG=""
          if [ "$XPU_NUM" -gt 0 ]; then
            for idx in $(seq 0 $((XPU_NUM-1))); do
              DOCKER_DEVICE_CONFIG="${DOCKER_DEVICE_CONFIG} --device=/dev/xpu${idx}:/dev/xpu${idx}"
            done
            DOCKER_DEVICE_CONFIG="${DOCKER_DEVICE_CONFIG} --device=/dev/xpuctrl:/dev/xpuctrl"
          fi
          BUILD_IMAGE="iregistry.baidu-int.com/xmlir/xmlir_ubuntu_2004_x86_64:v0.32"
          docker run -d ${DOCKER_DEVICE_CONFIG} \
            --name vllm-ut \
            --net=host \
            --cap-add=SYS_PTRACE --security-opt seccomp=unconfined \
            --tmpfs /dev/shm:rw,nosuid,nodev,exec,size=32g \
            -v "$PWD:/workspace" \
            -v /ssd1:/ssd1 \
            -v /ssd2:/ssd2 \
            -v /ssd3:/ssd3 \
            -v /usr/local/bin/xpu-smi:/usr/local/bin/xpu-smi \
            -w /workspace \
            "$BUILD_IMAGE" sleep infinity
      - name: Install Dependencies
        run: |
          docker exec vllm-ut bash -lc "
            conda activate python310_torch25_cuda
            

            pip install -r requirements.txt
            python setup.py build
            python setup.py develop

            wget -O xpytorch-cp310-torch251-ubuntu2004-x64.run https://baidu-kunlun-public.su.bcebos.com/v1/baidu-kunlun-share/1130/xpytorch-cp310-torch251-ubuntu2004-x64.run?authorization=bce-auth-v1%2FALTAKypXxBzU7gg4Mk4K4c6OYR%2F2025-12-02T05%3A01%3A27Z%2F-1%2Fhost%2Ff3cf499234f82303891aed2bcb0628918e379a21e841a3fac6bd94afef491ff7
            bash xpytorch-cp310-torch251-ubuntu2004-x64.run
            pip install "https://baidu-kunlun-public.su.bcebos.com/v1/baidu-kunlun-share/1130/xtorch_ops-0.1.2209%2B6752ad20-cp310-cp310-linux_x86_64.whl?authorization=bce-auth-v1%2FALTAKypXxBzU7gg4Mk4K4c6OYR%2F2025-12-05T06%3A18%3A00Z%2F-1%2Fhost%2F14936c2b7e7c557c1400e4c467c79f7a9217374a7aa4a046711ac4d948f460cd"
            pip install "https://cce-ai-models.bj.bcebos.com/v1/vllm-kunlun-0.11.0/triton-3.0.0%2Bb2cde523-cp310-cp310-linux_x86_64.whl"
            pip install "https://cce-ai-models.bj.bcebos.com/XSpeedGate-whl/release_merge/20251219_152418/xspeedgate_ops-0.0.0-cp310-cp310-linux_x86_64.whl"
            pip install vllm==0.11.0 --no-build-isolation --no-deps --index-url https://pip.baidu-int.com/simple/
            
            export http_proxy=http://10.63.229.53:8891
            export https_proxy=http://10.63.229.53:8891
            pip install pytest pytest-cov
          "
      - name: Run Unit Test
        run: |
          docker exec vllm-ut bash -lc "
            conda activate python310_torch25_cuda
            export XPU_VISIBLE_DEVICES=1
            pytest -vs \
              --cov=vllm_kunlun \
              --cov-report=term-missing \
              -p no:warnings tests/ut
          "
      - name: Close Docker
        if: always()
        run: |
          docker rm -f vllm-ut || true